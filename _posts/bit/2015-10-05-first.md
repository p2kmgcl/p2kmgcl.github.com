---
published: true
title: first
---

## Getting through a proxy with python2 HTTP

> If you want to create a python HTTP requests you will end using both httplib and urllib, as urllib provide many useful functions for standard HTTP requests (like urlencoding), but don't feel bad, is a normal thing.

If you need to pass through a proxy the easy way is to:

1. Specify `your.proxy.com` as host.
2. Specify your proxy port as port.
3. Specify `your.real.request:80/with/data?and=params` as path.
4. Send the request.

But if you need to make an authentication __don't__ specify `user:pass@your.proxy.com` as host, set `headers['proxy-authorization']` to:

    auth = 'Basic ' + b64encode('%s:%s' % (
        proxy_user, 
        proxy_password
    ))

## Spark stuff

### Nodes

Spark is prepared to run using multiple instances, in order to allow more than
a single machine access and process data. For each Spark system, there is a
single __master__ node and one or many __slave__ nodes. Communications between
nodes are performed through _http request_, so each node can be located
anywhere, as it has an accesible IP and port.

### Resilient Distributed Dataset

__RDD Facts__:
- They are lazy.
- They are inmutable (as big data).

RDD abstract distributed collections providing functions like _groupBy_, _join_
or _count_ letting us forget the MapReduce process is being done.

### Links

 - http://rahulkavale.github.io/blog/2014/11/16/scrap-your-map-reduce/
 
## About pom.xml

Those files are used in maven repositories for handling dependencies and, in general, project metadata (imagine a `package.js` file but in xml, because java does those kind of things).

If you need to deploy your code to multiple development environments the easiest way is to use `<profile>` tag overriding any configuration, and then using it with `mvn goal -Pprofile_name`. It simply works.

Oh! And if you need ssh deployment __don't__ use _wagon-ssh-external_, use _wagon-ssh_ instead. The first one seems to be a deprecated release and it is buggy.
