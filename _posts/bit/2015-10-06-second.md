---
published: true
title: Second
---

## [How to implement error codes in Python an Java (the standard way)](#error-codes)

### Python

	# Creating global variables (local to a concrete library)
    # is the common way. Keep names as tiny as possible.
	ERRNO1 = 0
    ERRNO2 = 1
    # ...

### Java

	class MyClass {
    	// In Java a public enumeration is the common way
    	public enum Error {
        	// Don't worry about huge names, Java
            // programmers use them everywhere
            MY_ERROR_NUMBER_ONE,
            MY_ERROR_NUMBER_TWO
            // ...
        }
    }

## [About Spark memory management](#spark-memory)

[Spark Programming Guide - RDDs](http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds)

 > __From Spark FAQ__: _Do I need Hadoop to run Spark?_
 > No, but if you run on a cluster, you will need some form of shared file system (for
 > example, NFS mounted at the same path on each node). If you have this type of
 > filesystem, you can just deploy Spark in standalone mode.

### Parallelizing

`sc.parallelize(data, partitions)` distributes your data around clusters in the number of partitions specified. If you do not specify the number of partitions it creates 2-4 partitions per CPU.

### External datasets

 > __Hadoop__ does this part, so you can use local files, HDFS, Cassandra...

Loading a file using `sc.textFile('file or folder or wildcard')` lets every CPU on the cluster access the file.

### Shared variables

You can use `sc.broadcast` and `sc.accumlator`, those elements will be available through the whole cluster.

## [SparkSQL (for python)](http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes)

Sparks uses drataframes for accessing structured data. This data can be loaded from a structured file (for example JSON) an accessed either using Spark's __SQLContext__ methods or standard SQL queries.

It also supports connecting to __Hive__ or external databases using __JDBC__. Apparently in Spark's documentation loading a single external table is the only thing you can do. If you want to use more complex data you should incorporate [__HiveContexts__](http://spark.apache.org/sql/) to the system.
